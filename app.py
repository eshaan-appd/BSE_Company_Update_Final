import os, io, re, time, tempfile
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
import numpy as np
from openai import OpenAI
import os, streamlit as st
# ---- OpenAI (Responses API) ----

api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
if not api_key:
    st.error("Missing OPENAI_API_KEY (set env var or add to Streamlit secrets).")
    st.stop()

client = OpenAI(api_key=api_key)

with st.expander("üîç OpenAI connection diagnostics", expanded=False):
    # 1) Is the key visible?
    key_src = "st.secrets" if "OPENAI_API_KEY" in st.secrets else "env"
    mask = lambda s: (s[:7] + "..." + s[-4:]) if s and len(s) > 12 else "unset"
    st.write("Key source:", key_src)
    st.write("API key (masked):", mask(st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")))

    # 2) Can we list models? (auth + project/perm sanity)
    try:
        _ = client.models.list()
        st.success("Models list ok ‚Äî auth + project look good.")
    except Exception as e:
        st.error(f"Models list failed: {e}")

    # 3) Can we call a tiny Responses echo? (billing/quota often shows up here)
    try:
        r = client.responses.create(model="gpt-4.1-mini", input="ping")
        st.success("Responses call ok.")
    except Exception as e:
        st.error(f"Responses call failed: {e}")

# =========================================
# Streamlit UI
# =========================================
st.set_page_config(page_title="BSE Company Update ‚Äî OpenAI PDF Summarizer", layout="wide")
st.title("üìà BSE Company Update ‚Äî M&A / Merger / Scheme / JV (OpenAI-only)")
st.caption("Fetch BSE announcements ‚Üí pick PDF ‚Üí upload to OpenAI ‚Üí summarize. No local NLP or OCR is used; summaries are generated by OpenAI models.")

# =========================================
# Small utilities
# =========================================
_ILLEGAL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]')
def _clean(s: str) -> str:
    return _ILLEGAL_RX.sub('', s) if isinstance(s, str) else s

def _first_col(df: pd.DataFrame, names):
    for n in names:
        if n in df.columns: return n
    return None

def _norm(s):
    return re.sub(r"\s+", " ", str(s or "")).strip()

def _slug(s: str, maxlen: int = 60) -> str:
    s = re.sub(r"[^A-Za-z0-9]+", "_", str(s or "")).strip("_")
    return (s[:maxlen] if len(s) > maxlen else s) or "file"

# =========================================
# Attachment URL candidates (unchanged)
# =========================================
def _candidate_urls(row):
    cands = []
    att = str(row.get("ATTACHMENTNAME") or "").strip()
    if att:
        cands += [
            f"https://www.bseindia.com/xml-data/corpfiling/AttachHis/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/Attach/{att}",
            f"https://www.bseindia.com/xml-data/corpfiling/AttachLive/{att}",
        ]
    ns = str(row.get("NSURL") or "").strip()
    if ".pdf" in ns.lower():
        cands.append(ns if ns.lower().startswith("http") else "https://www.bseindia.com/" + ns.lstrip("/"))
    seen, out = set(), []
    for u in cands:
        if u and u not in seen:
            out.append(u); seen.add(u)
    return out

# =========================================
# BSE fetch ‚Äî strict; returns filtered DF (unchanged logic)
# =========================================
def fetch_bse_announcements_strict(start_yyyymmdd: str,
                                   end_yyyymmdd: str,
                                   verbose: bool = True,
                                   request_timeout: int = 25) -> pd.DataFrame:
    """Fetches raw announcements, then filters:
    Category='Company Update' AND subcategory contains any:
    Acquisition | Amalgamation / Merger | Scheme of Arrangement | Joint Venture
    """
    assert len(start_yyyymmdd) == 8 and len(end_yyyymmdd) == 8
    assert start_yyyymmdd <= end_yyyymmdd
    base_page = "https://www.bseindia.com/corporates/ann.html"
    url = "https://api.bseindia.com/BseIndiaAPI/api/AnnSubCategoryGetData/w"

    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": base_page,
        "X-Requested-With": "XMLHttpRequest",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    })

    try:
        s.get(base_page, timeout=15)
    except Exception:
        pass

    variants = [
        {"subcategory": "-1", "strSearch": "P"},
        {"subcategory": "-1", "strSearch": ""},
        {"subcategory": "",   "strSearch": "P"},
        {"subcategory": "",   "strSearch": ""},
    ]

    all_rows = []
    for v in variants:
        params = {
            "pageno": 1, "strCat": "-1", "subcategory": v["subcategory"],
            "strPrevDate": start_yyyymmdd, "strToDate": end_yyyymmdd,
            "strSearch": v["strSearch"], "strscrip": "", "strType": "C",
        }
        rows, total, page = [], None, 1
        while True:
            r = s.get(url, params=params, timeout=request_timeout)
            ct = r.headers.get("content-type","")
            if "application/json" not in ct:
                if verbose: st.warning(f"[variant {v}] non-JSON on page {page} (ct={ct}).")
                break
            data = r.json()
            table = data.get("Table") or []
            rows.extend(table)
            if total is None:
                try: total = int((data.get("Table1") or [{}])[0].get("ROWCNT") or 0)
                except Exception: total = None
            if not table: break
            params["pageno"] += 1; page += 1; time.sleep(0.25)
            if total and len(rows) >= total: break
        if rows:
            all_rows = rows; break

    if not all_rows: return pd.DataFrame()

    all_keys = set()
    for r in all_rows: all_keys.update(r.keys())
    df = pd.DataFrame(all_rows, columns=list(all_keys))

    # Filter to Company Update
    def filter_announcements(df_in: pd.DataFrame, category_filter="Company Update") -> pd.DataFrame:
        if df_in.empty: return df_in.copy()
        cat_col = _first_col(df_in, ["CATEGORYNAME","CATEGORY","NEWS_CAT","NEWSCATEGORY","NEWS_CATEGORY"])
        if not cat_col: return df_in.copy()
        df2 = df_in.copy()
        df2["_cat_norm"] = df2[cat_col].map(lambda x: _norm(x).lower())
        return df2.loc[df2["_cat_norm"] == _norm(category_filter).lower()].drop(columns=["_cat_norm"])

    df_filtered = filter_announcements(df, category_filter="Company Update")
    df_filtered = df_filtered.loc[
        df_filtered.filter(["NEWSSUB","SUBCATEGORY","SUBCATEGORYNAME","NEWS_SUBCATEGORY","NEWS_SUB"], axis=1)
        .astype(str)
        .apply(lambda col: col.str.contains(r"(Acquisition|Amalgamation\s*/\s*Merger|Scheme of Arrangement|Joint Venture)", case=False, na=False))
        .any(axis=1)
    ]
    return df_filtered

# =========================================
# OpenAI PDF summarization
# =========================================
def _download_pdf(url: str, timeout=25) -> bytes:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/pdf,application/octet-stream,*/*",
        "Referer": "https://www.bseindia.com/corporates/ann.html",
        "Accept-Language": "en-US,en;q=0.9",
    })
    r = s.get(url, timeout=timeout, allow_redirects=True, stream=False)
    if r.status_code != 200:
        raise RuntimeError(f"HTTP {r.status_code}")
    data = r.content
    if not (data[:8].startswith(b"%PDF") or url.lower().endswith(".pdf")):
        # some BSE PDFs have no content-type; head check is enough
        pass
    return data

def _upload_to_openai(pdf_bytes: bytes, fname: str = "document.pdf"):
    # The Files API stores the PDF so a model can read it; then we attach it in a Responses call.
    # Using a temp file ensures a valid filename is sent.
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_bytes)
        tmp.flush()
        # purpose "assistants" is the general bucket for file inputs / tools
        f = client.files.create(file=open(tmp.name, "rb"), purpose="assistants")
    return f

def _render_bordered_table_from_json(json_text: str, key: str = "table"):
    """
    Safely render a bordered table from JSON-like text.
    Auto-cleans extra backticks, markdown fences, and stray text.
    """
    import json
    import pandas as pd
    import streamlit as st

    cleaned = (
        json_text.strip()
        .replace("```json", "")
        .replace("```", "")
        .replace("‚Äú", '"')
        .replace("‚Äù", '"')
        .replace("‚Äô", "'")
    )

    # Try to find the first '{' and last '}' to isolate valid JSON
    if "{" in cleaned and "}" in cleaned:
        cleaned = cleaned[cleaned.find("{"): cleaned.rfind("}") + 1]

    try:
        data = json.loads(cleaned)
        rows = data.get(key, [])
        if not rows:
            st.warning("No rows found in parsed JSON.")
            st.code(cleaned, language="json")
            return
        df = pd.DataFrame(rows)
        styled = (
            df.style
              .hide(axis="index")
              .set_table_styles([
                  {"selector": "table", "props": "border-collapse: collapse; border: 1px solid #bbb;"},
                  {"selector": "th", "props": "border: 1px solid #bbb; padding: 8px; background: #f6f7fb; text-align: left;"},
                  {"selector": "td", "props": "border: 1px solid #bbb; padding: 8px;"},
              ])
        )
        st.markdown(styled.to_html(), unsafe_allow_html=True)
    except Exception as e:
        st.warning(f"Could not parse model output as JSON ({e}). Showing raw text:")
        st.code(json_text)



def summarize_pdf_with_openai(pdf_bytes: bytes, company: str, headline: str, subcat: str,
                              model: str = "gpt-4.1-mini", style: str = "bullets", max_output_tokens: int = 800,
                              temperature: float = 0.2) -> str:
        """
        Uses the Responses API with a file attachment. The model reads the PDF and returns a summary.
        """
        fobj = _upload_to_openai(pdf_bytes, fname=f"{_slug(company or 'doc')}.pdf")
    
        # NOTE: JSON braces in an f-string must be escaped as {{ and }}

        task = f"""
        You are a meticulous compliance and regulatory analyst specializing in Indian listed company filings.
        Read the attached BSE/SEBI filing PDF carefully and produce a table with exactly three columns:
        
        1) Company
        2) Announcement Type From PDF
        3) Regulations
        
        Guidelines:
        - In "Company", copy the company name from the provided context below (do not infer a different legal name unless the PDF clearly states it).
        - In "Announcement Type From PDF", give a concise, specific title that best describes the nature of the announcement
          (e.g., "Outcome of Board Meeting", "Intimation of Board Meeting", "Record Date", "Dividend Declaration",
          "Investor Presentation", "Trading Window Closure", "Credit Rating", "Press Release", "RPT Disclosure",
          "Auditor Appointment", "KMP Change", "Buyback", "Preferential Issue / QIP", etc.).
        - In "Regulations":
          - If the PDF explicitly cites SEBI / LODR / PIT / Companies Act regulations, include the **exact text** (e.g., "Regulation 30 of SEBI (LODR) Regulations, 2015").
          - If no direct citation appears, infer the **most likely applicable regulation** based on the document‚Äôs content and standard practice. Examples:
              ‚Ä¢ Board meeting outcomes ‚Üí Regulation 30
              ‚Ä¢ Financial results ‚Üí Regulation 33
              ‚Ä¢ Shareholding pattern ‚Üí Regulation 31
              ‚Ä¢ Trading window closure ‚Üí SEBI (PIT) Regulations, Schedule B
              ‚Ä¢ Preferential issue / QIP ‚Üí SEBI (ICDR) Regulations (e.g., Reg. 164 for pricing)
              ‚Ä¢ Press release / investor presentation ‚Üí Regulation 30
              ‚Ä¢ Change in director / KMP ‚Üí Regulation 30
              ‚Ä¢ Dividend declaration ‚Üí Regulation 43
          - If multiple clearly apply, separate with semicolons; if no reasonable inference is possible, write "Not disclosed".
        
        Output format ‚Äî return ONLY valid JSON with this exact structure (no prose, no markdown):
        
        {{
          "table": [
            {{
              "Company": "{company or 'NA'}",
              "Announcement Type From PDF": "<announcement type>",
              "Regulations": "<exact or inferred regulation(s)>"
            }}
          ]
        }}
        
        Context:
        Company: {company or 'NA'}
        Headline: {headline or 'NA'}
        Subcategory: {subcat or 'NA'}
        """


    
        resp = client.responses.create(
            model=model,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            input=[{
                "role": "user",
                "content": [
                    {"type": "input_text", "text": task},
                    {"type": "input_file", "file_id": fobj.id},
                ],
            }],
        )
        return (resp.output_text or "").strip()
# Simple rate-limit friendly wrapper
def safe_summarize(*args, **kwargs) -> str:
    for i in range(4):
        try:
            return summarize_pdf_with_openai(*args, **kwargs)
        except Exception as e:
            msg = str(e)
            if "429" in msg or "rate" in msg.lower():
                time.sleep(2.0 * (i + 1))
                continue
            raise
    return "‚ö†Ô∏è Unable to summarize due to repeated rate limits."

# =========================================
# Sidebar controls
# =========================================
with st.sidebar:
    st.header("‚öôÔ∏è Controls")
    today = datetime.now().date()
    start_date = st.date_input("Start date", value=today - timedelta(days=1), max_value=today)
    end_date   = st.date_input("End date", value=today, max_value=today, min_value=start_date)

    model = st.selectbox(
        "OpenAI model",
        ["gpt-4.1-mini", "gpt-4o-mini", "gpt-4.1"],
        index=0,
        help="Models with vision/file-reading capability. 4.1-mini/4o-mini are cost-efficient."
    )
    style = st.radio("Summary style", ["bullets", "narrative"], horizontal=True)
    max_tokens = st.slider("Max output tokens", 200, 2000, 800, step=50)
    temperature = st.slider("Creativity (temperature)", 0.0, 1.0, 0.2, step=0.1)

    max_workers = st.slider("Parallel summaries", 1, 8, 3, help="Lower if you hit 429s.")
    max_items = st.slider("Max announcements to summarize", 5, 200, 60, step=5)

    run = st.button("üöÄ Fetch & Summarize with OpenAI", type="primary")

# =========================================
# Run pipeline (fetch ‚Üí PDFs ‚Üí OpenAI summaries)
# =========================================
def _fmt(d: datetime.date) -> str: return d.strftime("%Y%m%d")

def _pick_company_cols(df: pd.DataFrame) -> tuple[str, str]:
    nm = _first_col(df, ["SLONGNAME","SNAME","SC_NAME","COMPANYNAME"]) or "SLONGNAME"
    subcol = _first_col(df, ["SUBCATEGORYNAME","SUBCATEGORY","SUB_CATEGORY","NEWS_SUBCATEGORY"]) or "SUBCATEGORYNAME"
    return nm, subcol

if run:
    if not os.getenv("OPENAI_API_KEY"):
        st.error("Missing OPENAI_API_KEY (set env var, add to Streamlit Secrets, or export in your shell).")
        st.stop()

    start_str, end_str = _fmt(start_date), _fmt(end_date)

    with st.status("Fetching announcements‚Ä¶", expanded=True):
        df_hits = fetch_bse_announcements_strict(start_str, end_str, verbose=False)
        st.write(f"Matched rows after filters: **{len(df_hits)}**")

    if df_hits.empty:
        st.warning("No matching announcements in this window.")
        st.stop()

    if len(df_hits) > max_items:
        df_hits = df_hits.head(max_items)

    # Build list of PDF targets
    rows = []
    for _, r in df_hits.iterrows():
        urls = _candidate_urls(r)
        rows.append((r, urls))

    st.subheader("üìë Summaries (OpenAI)")
    nm, subcol = _pick_company_cols(df_hits)

    # Worker to download and summarize a single row
    def worker(idx, row, urls):
        # try urls in order until one downloads
        pdf_bytes, used_url = None, ""
        for u in urls:
            try:
                data = _download_pdf(u, timeout=25)
                if data and len(data) > 500:
                    pdf_bytes, used_url = data, u
                    break
            except Exception:
                continue
        if not pdf_bytes:
            return idx, used_url, "‚ö†Ô∏è Could not fetch a valid PDF.", None

        company = str(row.get(nm) or "").strip()
        headline = str(row.get("HEADLINE") or "").strip()
        subcat = str(row.get(subcol) or "").strip()

        summary = safe_summarize(pdf_bytes, company, headline, subcat,
                                 model=model,
                                 style=("bullets" if style=="bullets" else "narrative"),
                                 max_output_tokens=int(max_tokens),
                                 temperature=float(temperature))
        return idx, used_url, summary, None

    # Run with limited parallelism
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futs = [ex.submit(worker, i, r, urls) for i, (r, urls) in enumerate(rows)]
        for fut in as_completed(futs):
            i, pdf_url, summary, _ = fut.result()
            r = rows[i][0]
            company = str(r.get(nm) or "").strip()
            dt = str(r.get("NEWS_DT") or "").strip()
            subcat = str(r.get(subcol) or "").strip()
            headline = str(r.get("HEADLINE") or "").strip()

            with st.expander(f"{company or 'Unknown'} ‚Äî {dt}  ‚Ä¢  {subcat or 'N/A'}", expanded=False):
                if headline:
                    st.markdown(f"**Headline:** {headline}")
                if pdf_url:
                    st.markdown(f"[PDF link]({pdf_url})")
                _render_bordered_table_from_json(summary, key="table")

else:
    st.info("Pick your date range and click **Fetch & Summarize with OpenAI**. This version uploads each PDF to OpenAI and renders the model‚Äôs summary right here.")
